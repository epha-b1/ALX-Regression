{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Code_challenge.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Project: Understanding the yield\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coding challenge, we will apply all of the skills we learned in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Note that this code challenge is graded and will contribute to your overall marks for this module. Submit this notebook for grading. Note that the names of the functions are different in this notebook. Transfer the code in your notebook to this submission notebook**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- **Do not add or remove cells in this notebook. Do not edit or remove the `### START FUNCTION` or `### END FUNCTION` comments. Do not add any code outside of the functions you are required to edit. Doing any of this will lead to a mark of 0%!**\n",
    "\n",
    "- Answer the questions according to the specifications provided.\n",
    "\n",
    "- Use the given cell in each question to see if your function matches the expected outputs.\n",
    "\n",
    "- Do not hard-code answers to the questions.\n",
    "\n",
    "- The use of StackOverflow, Google, and other online tools is permitted. However, copying a fellow student's code is not permissible and is considered a breach of the Honour code. Doing this will result in a mark of 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression is a fundamental statistical method used to quantify the relationship between two variables. It allows us to predict an outcome (dependent variable) based on the value of one predictor (independent variable). In this challenge, we will apply simple linear regression to understand how different environmental factors affect the standardised yield of crops.\n",
    "\n",
    "Our insights will not only help local farmers maximise their harvests but also contribute to the sustainable agriculture practices in Maji Ndogo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we sow the seeds of our regression model, we need to get to know our soil – the dataset. This dataset was developed through extensive agricultural surveys conducted at farms across Maji Ndogo. It contains various factors that might influence a farm's crop yield, from the elevation of the fields to the average temperature they bask in.\n",
    "\n",
    "Spend some time looking at the data dictionary and start thinking about what could be influencing our crop yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float). (DUMMY VARIABLE)\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield (DUMMY VARIABLE - Removed)\n",
    "\n",
    "<br>\n",
    "\n",
    "**5. Target variable**\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our database again, like we did last time. We won't use the weather data so it is commented out.\n",
    "\n",
    "**Important:** Ensure that `data_ingestion.py` file and the `field_data_processor.py` files are stored in the same folder as your notebook, otherwise the data import will fail.\n",
    " \n",
    "[Download files here](https://github.com/Explore-AI/Public-Data/raw/master/Maji_Ndogo/modules.zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 16:10:46,242 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-03-13 16:10:46,245 - data_ingestion - ERROR - An error occurred while querying the database. Error: (sqlite3.DatabaseError) file is not a database\n",
      "[SQL: \n",
      "            SELECT *\n",
      "            FROM geographic_features\n",
      "            LEFT JOIN weather_features USING (Field_ID)\n",
      "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
      "            LEFT JOIN farm_management_features USING (Field_ID)\n",
      "            ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/4xp6)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "(sqlite3.DatabaseError) file is not a database\n[SQL: \n            SELECT *\n            FROM geographic_features\n            LEFT JOIN weather_features USING (Field_ID)\n            LEFT JOIN soil_and_crop_features USING (Field_ID)\n            LEFT JOIN farm_management_features USING (Field_ID)\n            ]\n(Background on this error at: https://sqlalche.me/e/20/4xp6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1965\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1965\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\default.py:921\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 921\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: file is not a database",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Ignoring the field data for now.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m field_processor \u001b[38;5;241m=\u001b[39m FieldDataProcessor(config_params)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mfield_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m field_df \u001b[38;5;241m=\u001b[39m field_processor\u001b[38;5;241m.\u001b[39mdf\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# We're not going to use the weather data this time, so we'll ignore it.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# weather_processor = WeatherDataProcessor(config_params)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# weather_processor.process()\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# weather_df = weather_processor.weather_df\u001b[39;00m\n",
      "File \u001b[1;32md:\\#file\\Learning material\\ALX\\DataScience\\Course\\Regression\\Project\\field_data_processor.py:183\u001b[0m, in \u001b[0;36mFieldDataProcessor.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    Applies corrections to the DataFrame based on mappings specified in values_to_rename.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    - The DataFrame with corrections applied to the specified column.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_sql_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrename_columns()\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_corrections()\n",
      "File \u001b[1;32md:\\#file\\Learning material\\ALX\\DataScience\\Course\\Regression\\Project\\field_data_processor.py:110\u001b[0m, in \u001b[0;36mFieldDataProcessor.ingest_sql_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03mIngests data from a SQL database based on the configured database path and SQL query.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m- A pandas DataFrame containing the data fetched from the database.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m create_db_engine(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mquery_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSucessfully loaded data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\#file\\Learning material\\ALX\\DataScience\\Course\\Regression\\Project\\data_ingestion.py:79\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     78\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\#file\\Learning material\\ALX\\DataScience\\Course\\Regression\\Project\\data_ingestion.py:66\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[1;32m---> 66\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Log a message or handle the empty DataFrame scenario as needed\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03mparameter will be converted to UTC.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    396\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[1;32m-> 1560\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1561\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectable\u001b[38;5;241m.\u001b[39mexecution_options()\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1412\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\sql\\elements.py:516\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[1;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1635\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[1;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[0;32m   1623\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[0;32m   1625\u001b[0m )\n\u001b[0;32m   1627\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[0;32m   1628\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[0;32m   1629\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1633\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[0;32m   1634\u001b[0m )\n\u001b[1;32m-> 1635\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[0;32m   1648\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[0;32m   1649\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1650\u001b[0m         elem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         ret,\n\u001b[0;32m   1655\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1844\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(\n\u001b[0;32m   1840\u001b[0m         dialect,\n\u001b[0;32m   1841\u001b[0m         context,\n\u001b[0;32m   1842\u001b[0m     )\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1984\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1981\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[0;32m   1983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1984\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2339\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2338\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1965\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1963\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1965\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1971\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1972\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1976\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1977\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\default.py:921\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 921\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: (sqlite3.DatabaseError) file is not a database\n[SQL: \n            SELECT *\n            FROM geographic_features\n            LEFT JOIN weather_features USING (Field_ID)\n            LEFT JOIN soil_and_crop_features USING (Field_ID)\n            LEFT JOIN farm_management_features USING (Field_ID)\n            ]\n(Background on this error at: https://sqlalche.me/e/20/4xp6)"
     ]
    }
   ],
   "source": [
    "# Read the database, and clean the data using the processing modules we built.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "# from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
    "    \"regex_patterns\" : {\n",
    "            'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "            'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "            'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "            },\n",
    "}\n",
    "# Ignoring the field data for now.\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "# We're not going to use the weather data this time, so we'll ignore it.\n",
    "# weather_processor = WeatherDataProcessor(config_params)\n",
    "# weather_processor.process()\n",
    "# weather_df = weather_processor.weather_df\n",
    "\n",
    "dataset = field_df.drop(\"Weather_station\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into our analysis, it's crucial to ensure the integrity of our dataset and that the data is still as we expect it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the data\n",
    "# !pip install pytest\n",
    "\n",
    "dataset.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "!pytest validate_data.py -v\n",
    "\n",
    "import os# Define the file paths\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Visualising the relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data ready and loaded, it's time to start exploring. \n",
    "\n",
    "Our goal is to determine whether any of the features in our dataset are influencing the `Standard_yield` of a farm. If we can figure out what these relationships are, then we can use them to start predicting what future yields will be, based on these features. \n",
    "\n",
    "For this analysis, we want to find whether any features have a linear relationship with `Standard_yield` so that we can fit a linear regression model to the data. This is important because if we try and fit a linear regression model to non-linear data, our predictions won't be good.\n",
    "\n",
    "Any of the features could have an impact on the `Standard_yield`. Let's begin with `Ave_temps`, the average temperature of the region, and its relationship to `Standard_yield`.\n",
    "\n",
    "Let's start with the basics: a scatter plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚙️ Your task:**\n",
    "\n",
    " 1. Generate a scatter plot to visualise the relationship between `Ave_temps` and `Standard_yield`.\n",
    " 2. Reflect on the scatter plot. Does it suggest a linear relationship, or is the story more complex?\n",
    "\n",
    "**Note:**\n",
    "- Use `matplotlib` to create the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "  \n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(dataset['Ave_temps'], dataset['Standard_yield'])\n",
    "plt.xlabel('Ave_temps')\n",
    "plt.ylabel('Standard_yield')\n",
    "plt.title('Relationship between average temperature and yield')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a function to calculate the Pearson correlation coefficient.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `get_correlation` that:\n",
    "1. Takes a DataFrame and the names of the columns we want to determine the correlation for as parameters (`Ave_temps` and `Standard_yield`).\n",
    "2. Calculates the Pearson correlation coefficient between these two columns to quantify their linear relationship.\n",
    "4. Returns the Pearson correlation coefficient.\n",
    "\n",
    "**Note:**\n",
    "- Use `scipy` to calculate the Pearson correlation coefficient.\n",
    "- Ensure your function returns the Pearson correlation coefficient as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def get_correlation(df, col1, col2):\n",
    "    correlation, _ = pearsonr(df[col1], df[col2])\n",
    "    \n",
    "    return correlation\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = get_correlation(dataset,'Ave_temps','Standard_yield')\n",
    "print(\"Pearson correlation coefficient:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "Correlation: 0.006785950289020164\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the scatter plot and the dispersion of data points? It's essential to visualise our data first; if the data doesn't follow a linear pattern, then a linear regression model may fail to accurately capture the underlying relationship. The correlation also seems extremly low, what does this tell us?\n",
    "\n",
    "Let's write down some of our observations:\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: A breath of fresh data: Pollution as a predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to shift our gaze from the warmth of the sun to the haze of pollution. Could the levels of pollution, a concern for farmers and environmentalists alike, be an indicator of our yields?\n",
    "\n",
    "Let's begin by fitting a simple linear regression model, to try and capture the linear relationship between these columns.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `fit_linear_regression_model` that:\n",
    "1. Takes in a DataFrame and the names of the `Pollution_level` and `Standard_yield` columns.\n",
    "2. Fits a linear regression model to the data.\n",
    "3. Returns the model, the model predictions and the actual y-values.\n",
    "\n",
    "**Note:**\n",
    "- Use `LinearRegression` from `sklearn` to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def fit_linear_regression_model(df, pollution_col, yield_col):\n",
    "    X = df[[pollution_col]]\n",
    "    y = df[yield_col]\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    return model, predictions, y\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predictions, y_values = fit_linear_regression_model(dataset, 'Pollution_level', 'Standard_yield')\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Actual Y-Values: {y_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "- Model: an instance of the LinearRegression class.\n",
    "- Predictions: a NumPy array of predicted values.\n",
    "- y: a Pandas Series with the actual target values used for training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression models only work well if our data is in fact linear. So, lets create a scatter plot to visualise the relationship between pollution and crop yields. In addition to this, let's use the predictions from the model we fit to add the line of best fit to our scatter plot.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Generate a scatter plot to visualise the effect that pollution has on standard yield.\n",
    "2. Draw the line of best fit\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "You can use this line of code to draw the regression line on the plot:\n",
    "`plt.plot(X, predictions, color='red', label='Regression line')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['Pollution_level']]\n",
    "y = dataset['Standard_yield']\n",
    "\n",
    "plt.scatter(X, y, label='Data points')\n",
    "plt.plot(X, predictions, color='red', label='Regression line')\n",
    "plt.xlabel('Pollution_level')\n",
    "plt.ylabel('Standard_yield')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the get_correlation() function that we defined earlier to test the correlation between `Pollution_level` and `Standard_yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pollution_correlation = get_correlation(dataset,'Pollution_level','Standard_yield')\n",
    "print(\"Pearson correlation coefficient:\", Pollution_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```\n",
    "Correlation: -0.2857609646210543\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on the difference between this plot and correlation and the previous one with the average temperature. Is the relationship between pollution and yield more linear?\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also gain a better understanding of our model by examining the slope and intercept.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `get_slope_intercept` that:\n",
    "1. Inputs the `model` we fitted and calculates the slope and intercept of the line of best fit.\n",
    "2. Return the slope and intercept as a tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def get_slope_intercept(model):\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    return slope, intercept\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = get_slope_intercept(model)\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "Slope: -0.1427617720986604\n",
    "Intercept: 0.5662684415393379\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What does the slope tell us about the strength of the relationship between pollution and yield? Also, what can we learn from the y-intercept?\n",
    "\n",
    "  - ✍️ Your notes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: The haze clears: Evaluating pollution's predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the scatterplots of `Standard_yield` with `Ave_temps` and `Pollution_level`, it appears that pollution level might have a more linear relationship.  This means that we could potentially use a simple linear regression model to make predictions about the yield of a farm based on its pollution level. However, before we do this we need to further assess the strength of the linear relationship between `Pollution_level` and `Standard_yield`. \n",
    "\n",
    "Let's assess our model's performance using R-squared, Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `calculate_evaluation_metrics` that:\n",
    "1. Takes the predictions and y-values from our fitted model as input.\n",
    "2. Calculates and returns the R-squared, Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) of the model's performance.\n",
    "\n",
    "**Note:**\n",
    "1. Calculate the model's performance metrics using the entire dataset.\n",
    "2. Return the evaluation metrics as a tuple in the order: R-squared, MAE, MSE, RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def calculate_evaluation_metrics(predictions, y_values):\n",
    "    \n",
    "    r2 = r2_score(y_values, predictions)\n",
    "    mae = mean_absolute_error(y_values, predictions)\n",
    "    mse = mean_squared_error(y_values, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return r2, mae, mse, rmse\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = calculate_evaluation_metrics(predictions, y_values)\n",
    "print(f\"Evaluation Metrics:\\nR-squared: {evaluation_metrics[0]}\\nMAE: {evaluation_metrics[1]}\\nMSE: {evaluation_metrics[2]}\\nRMSE: {evaluation_metrics[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "A tuple containing numerical values for R-squared, MAE, MSE, and RMSE (give or take 0.0001):\n",
    "\n",
    "```python\n",
    "R-squared: 0.08165932890115546\n",
    "MAE: 0.08554642090904992\n",
    "MSE: 0.011477732254034848\n",
    "RMSE: 0.10713417873878928\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what these metrics tell us about our model's accuracy and reliability. Write down your observations:\n",
    "\n",
    "  - ✍️ Your notes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: The dividing line: Train-test split in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we delve deeper into the relationship between `Pollution_level` and `Standard_yield`, we must ensure our model is not merely memorising the data but truly understanding it. This brings us to the pivotal technique of Train-Test Split.\n",
    "\n",
    "**The importance of train-test split**\n",
    "\n",
    "Imagine teaching a student for an exam by using the very questions that will appear on it. They might score perfectly, but does it mean they've truly learned? Similarly, a model might perform exceptionally on the data it was trained on, but the real test of knowledge comes from unseen data. This is where the train-test split comes in, allowing us to assess our model's generalisation capabilities by training on one subset of data and testing on another.\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Create a function named `data_train_test_split` that:\n",
    "1. Takes in the DataFrame and the two columns we want to model the relationship between (`Pollution_level` and `Standard_yield`).\n",
    "2. Separates it into features (`X`) based on `Pollution_level` and the target (`y`) based on `Standard_yield`.\n",
    "3. Splits the data into training and testing sets using an 80-20 split and sets `random_state = 42` for reproducibility.\n",
    "4.  Returns a tuple containing: `X_train` and `X_test`, which are DataFrames containing features for training and testing, respectively, along with `y_train` and `y_test`, which are Series representing subsets of the original DataFrame's target variable for training and testing.\n",
    "\n",
    "**Note:**\n",
    "- Use `train_test_split` from `sklearn.model_selection` to split the data.\n",
    "- Train a linear regression model on the training set using `LinearRegression` from `sklearn.linear_model`.\n",
    "- If the random state is not set to `42` the code will not be marked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def data_train_test_split(df, pollution_col, yield_col): \n",
    "    X = df[[pollution_col]] \n",
    "    y = df[yield_col] \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "    return X_train, X_test, y_train, y_test \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_train_test_split(dataset, 'Pollution_level', 'Standard_yield')\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "- X_train: DataFrame, subset of the original DataFrame's features for training.\n",
    "- X_test: DataFrame, subset of the original DataFrame's features for testing.\n",
    "- y_train: Series, subset of the original DataFrame's target variable for training.\n",
    "- y_test: Series, subset of the original DataFrame's target variable for testing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit a linear regression model to the data.\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Create a function named `train_split_linear_regression_model()` that:\n",
    "1. Takes `X_train`, `X_test`, `y_train`, `y_test` as input (the results from the `data_train_test_split()` function).\n",
    "2. Trains a simple linear regression model on the training set.\n",
    "3. Uses the testing set to make predictions.\n",
    "4. Returns a tuple containing: the model, the predictions and y_test (the actual y values in the testing set values) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def train_split_linear_regression_model(X_train, X_test, y_train, y_test): \n",
    "    model = LinearRegression().fit(X_train, y_train) \n",
    "    predictions = model.predict(X_test) \n",
    "    return model, predictions, y_test\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_model, predictions_test, y_test = train_split_linear_regression_model(X_train, X_test, y_train, y_test)\n",
    "print(f\"Train-Test Model: {train_test_model}\")\n",
    "print(f\"Test Predictions: {predictions_test}\")\n",
    "print(f\"Test Actual Y-Values: {y_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "- Model: an instance of the LinearRegression class.\n",
    "- Predictions: a NumPy array of predicted values.\n",
    "- y_test: a Pandas Series with the actual target values used for evaluating the model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate our use our train-test model by determining R-squared, MAE, MSE, and RMSE.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Use the `calculate_evaluation_metrics` function (defined in Challenge 3) to calculate the R-squared, MAE, MSE, and RMSE.\n",
    "2. The function should return a tuple containing the evaluation metrics (R-squared, MAE, MSE, and RMSE).\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Ensure to use the test set to calculate the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "A tuple containing numerical values for R-squared, MAE, MSE, and RMSE (give or take 0.0001):\n",
    "\n",
    "```python\n",
    "R-squared: 0.08065722992150859\n",
    "MAE:  0.08794942119747501\n",
    "MSE: 0.012250634233355654\n",
    "RMSE: 0.11068258324305434\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on the difference between these metics and the metrics we obtained from the previous model (that was not split into training and testings sets). Why do you think the fit is worse now? And, why should we choose the worse option? (Reflect on the course material if these answers to these questions are not clear.)\n",
    "\n",
    "  - ✍️ Your notes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Diagnosing model fit through residual analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis, it seems as though neither `Ave_temps` or `Pollution_level` have a strong linear fit with `Standard_yield`. However, even if we had obtained good results from our evaluation metrics, there are still other crucial assumptions we need to verify to ensure our model is well-fitted. Residual analysis plays a pivotal role in diagnosing the fit of linear regression models, helping us understand whether the assumptions of linearity, independence, and homoscedasticity (constant variance) of residuals are met. \n",
    "\n",
    "If they are not met, can we confidently model this problem using the model? And why?\n",
    "\n",
    "  - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a histogram.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Calucate the residuals of our train test model (difference between `y_test` and `predictions_test`)\n",
    "2. Plot these residuals as a histogram to assess their distribution and identify any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  Draw the histogram of residuals\n",
    "residuals = y_test - predictions_test\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=20, edgecolor='black')\n",
    "plt.title('Distribution of residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the histogram tell us about our data:\n",
    "\n",
    "1. Examine the shape of the histogram. What does the distribution of residuals tell us about the normality of the data? Consider whether the residuals appear to be symmetrically distributed around zero.\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "2. Compare the tails of the histogram to a normal distribution. Are there signs of heavy tails or skewness that could affect the reliability of the regression model's predictions?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "3. Assess the centering of the histogram around the zero line. How does this central tendency reflect on the bias of the model's predictions?\n",
    "\n",
    "    - ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a scatter plot of these residuals against the predicted values. \n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "1. Create a scatter plot of the residuals against the predicted values - `predictions_test` should be on the x-axis and the `residuals` on the y-axis.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- A horizontal line can be added at y=0 to make it easier to see if the residuals are evenly distributed around zero by adding this line of code:\n",
    "`plt.axhline(y=0, color='r', linestyle='--')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Draw the scatter plot of residuals against the predicted values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(predictions_test, residuals, edgecolor='black')\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relfect on what the scatter plot tells us about our data and the fit of the model:\n",
    "\n",
    "1. Analyse the scatter plot for any apparent patterns or structures in the data. What does this suggest about the appropriateness of the linear regression model for the dataset?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "2. Inspect the plot for signs of heteroscedasticity. How does the spread of residuals change as the predicted values increase? What might this imply about the constant variance assumption in linear regression?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "3. Identify whether the residuals are evenly scattered above and below the zero line across the range of predicted values. What can this tell us about the model's performance in terms of bias and prediction accuracy?\n",
    "\n",
    "    - ✍️ Your notes here\n",
    "\n",
    "4. Look for outliers or clusters of points that deviate significantly from the majority. How might these points influence the overall fit of the model?\n",
    "\n",
    "    - ✍️ Your notes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final task is to examine the mean and standard deviation of the residuals, which provide further insights into the model's performance.\n",
    "\n",
    "**⚙️ Your task:**\n",
    "\n",
    "Create a function named `calculate_residuals_statistics` that:\n",
    "1. Uses the `predictions_test` and `y_test` (obtained from Challenge 4) to calculate the residuals.\n",
    "2. Calculates the mean and standard deviation of the residuals.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Use `numpy` for the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def calculate_residuals_statistics(predictions, y_test):\n",
    "    residuals = y_test - predictions\n",
    "    mean_residual = np.mean(residuals)\n",
    "    std_residual = np.std(residuals)\n",
    "    return mean_residual, std_residual\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_residual, std_residual = calculate_residuals_statistics(predictions_test, y_test)\n",
    "print(f\"Mean: {mean_residual}\\nStandard deviation: {std_residual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```\n",
    "Mean: 0.0058580231923217015\n",
    "Standard deviation: 0.11052745268770957\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the mean of the residuals tell us about the bias in our predictions? How does a mean close to zero reflect on our model's accuracy?\n",
    "\n",
    "- ✍️ Your notes here\n",
    "\n",
    "What does the standard deviation of the residuals indicate about the variability of our predictions? Why is it important for this value to be relatively low?\n",
    "\n",
    " - ✍️ Your notes here\n",
    "\n",
    "What are the potential consequences of a high standard deviation of residuals on the reliability of the model's predictions? How might this affect our confidence in the model's estimates?\n",
    "\n",
    "- ✍️ Your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Linear regression, for all its strengths, assumes a straightforward relationship between the predictor and the outcome. Yet, the natural world seldom adheres to such simplicity. Factors influencing crop yields in Maji Ndogo—be it temperature, rainfall, or pollution—interact in complex, often nonlinear ways. Our initial model with `Ave_temps` hinted at this complexity, suggesting that the effect of the average temperature on yields might follow a more intricate pattern than a straight line can depict (or no pattern at all).\n",
    "\n",
    "Our yield also depends on more than just the pollution or the temperature, it depends on many of the factors. From our EDA we could see that. We also saw that not all crops are affected equally by pollution or temperature, so we could simplify our model if we remove the influence of the different crops. Once your submission is done, as a challenge to yourself, try to split the data again by crop type (with a loop) and use the functions you created to loop over all of the crop types and print out your metrics.\n",
    "\n",
    "Compare them, and discuss your results with your colleagues. Is there a crop type that is affected by pollution more than other crop types?\n",
    "\n",
    "As we dive deeper into regression, it's crucial to remember that with each model comes a new perspective. Just as a farmer selects the tool that best suits the task at hand, so must we choose our models with intention and insight. Exploring beyond linear regression opens up new vistas of understanding, allowing us to capture the richness of relationships within our data.\n",
    "\n",
    "In the fields of Maji Ndogo and beyond, countless stories await. It's up to us, with curiosity as our guide and an ever-expanding array of models at our disposal, to uncover them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExploreDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
